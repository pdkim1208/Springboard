Data Wrangling for Capstone 1	Two different datasets, NFL_Draft_Rounds.csv and CombineMeasurables.csv, are being used for analysis. CombineMeasurables.csv contains all the data around the performance of each player at the draft, while NFL_Draft_Rounds.csv contains the Round that each player has been drafted in. CombineMeasuarable.csv contains data from 1987 to 2017, while NFL_Draft_Rounds contains data from 1985.      CombineMeasurables contains the following columns:      Year, Name, College, POS, Height, Weight, Wonderlic, 40_Yard, Bench_Press, Vert_Leap, Broad_Jump, Shuttle, 3Cone            NFL_Draft_Rounds containts the following columns:      Player_Id, Year, Rnd, Pick, Tm, Player, Pos, Position Standard, First4AV, Age, To, AP1, PB, St, CarAV, DrAV, G, Cmp, Pass_Att, Pass_Yds, Pass_Int, Rush_Att, Rush_Yds, Rush_TDs, Rec, Rec_Yds, Rec_Tds, Tkl, Def_Int, Sk, College/Univ.      	The first major item of cleanup was dropping the columns that were outside the scope of the project. Since the purpose of the project was to build a prediction model around combine performance and not college statistics, we will drop all columns containing game statistics. We executed this as such, after assigning combine_measurables to CombineMeasurables.csv and rounds to NFL_Draft_Rounds.csv:combine_measurables = combine_measurables[['Year', 'Name', 'POS', 'Height', 'Weight', 'Wonderlic', '40_Yard', 'Bench_Press', 'Vert_Leap', 'Broad_Jump', 'Shuttle', '3Cone']]#We only want the following columns from rounds - Player, Year, Round, and Posrounds = rounds[['Player', 'Year', 'Rnd', 'Pos']]	To make the join operation easier (we will be doing a left join in order to attach the round in which the player was selected to the combine the the round dataframe), we renamed columns in the combine_measurables dataframe to match that of the rounds dataframe as such:combine_measurables = combine_measurables.rename(index = str, columns = {'Name': 'Player', 'POS':'Pos'})	Next, we need to cut off entries from the rounds dataframe that contains values from any year before 1987, which is the first year that official NFL Combine data is available, by doing executing the following:rounds = rounds[rounds['Year'] >= 1987]      The NFL Draft used be 12 rounds instead of today’s 7 – in order to standardize the data, we can execute the following in the rounds dataframe:            rounds = rounds[rounds['Rnd'] <= 7]       A very important component of wrangling the datasets involves understanding the different position. We can use nuniques() and uniques() on the “Position” column to obtain these values. Because we will be performing a join operation on these datasets with position as one of the “on” columns, this step is very crucial. What I did was consolidate position groups into a superset, using standard supersets of position groups with similar functions, weight, height, and measurable. C, G, LS, T, OL, OT  -->  OLDE, DL, DT, NT  -->  DLCB, DB, SS, FS  -->  DB      The method below can be used on the ‘Pos’ column of both dataframes to convert each position group to its proper supersetdef consolidate(i):    if i == 'CB' or i == 'FS' or i == 'SS':        return 'DB'    elif i == 'ILB' or i == 'OLB':        return 'LB'    elif i == 'NT' or i == 'DE' or i == 'DT':        return 'DL'    elif i == 'C' or i == 'LS' or i == 'G' or i == 'T' or i == 'OT':        return 'OL'    else:        return str(i)	A minor consideration was the elimination of punters (P) and kickers (K), as they both have a very small sample size, largely null values, and a very low correlation between NFL combine measurable with NFL success (as they are niche skill positions). We can eliminate them from both dataframes with the following:rounds = rounds[rounds['Pos'] != 'K']rounds = rounds[rounds['Pos'] != 'P']combine_measurables = combine_measurables[combine_measurables['Pos'] != 'K']combine_measurables = combine_measurables[combine_measurables['Pos'] != 'P']	Once both dataframes are ready, we will execute the left join to append the rounds in which each player was drafted to the combine data. We need to keep the NaN values (which is why we use the left join), as we can later change these to Undrafted.		df_joined = combine_measurables.merge(rounds, how = 'left', on = ['Year', 'Player', 'Pos'])	We drop the columns ‘Year’ and ‘Player’, as we would not be making any analysis by those dimensions.      df_joined = df_joined[['Rnd', 'Pos', 'Height', 'Weight', 'Wonderlic', '40_Yard', 'Bench_Press', 'Vert_Leap', 'Broad_Jump', 'Shuttle', '3Cone']]	In order to reduce the number of dimensions to make the data more comprehensible, we consolidate the rounds that the players were drafted in as a string in the following groups (1-3, 4-7, Undrafted):def consolidate_rounds(i):    if i == 1 or i == 2 or i == 3:        return '1-3'    elif i == 4 or i == 5 or i == 6 or i == 7:        return '4-7'    else:        return 'Undrafted'	An aspect of the NFL Combine is that prospects don’t have to complete all of the drills. For example, Deion Sanders, a high-profile first-round draft pick and first ballot Hall of Famer, famously arrived in a limo, ran the 40-yard dash, saluted the scouts, then got back into his limo and back to his private plane. Thus, he only had one measurable, with all the rest as NA. We deal with this unavailable data using dropna(), but we do this once we do a more in-depth analysis. For example, when creating a violinplot for forty yard dash times grouped by position group, we execute the following (we use dropna() after creating the new dataframe):#Create a violinplot for 40 Yard dash times, grouped by positionforty_yard_df_pos = df_joined[['Pos', '40_Yard']].dropna()sns.set()_ = sns.violinplot('Pos', '40_Yard', data = forty_yard_df_pos)_ = plt.xlabel('Position Group')_ = plt.ylabel('40-Yard Dash Times')_ = plt.title('40-Yard Dash Times By Position Group')_ = plt.show()	There weren’t very many outliers, as the NFL has a selection committee to determine which prospects are likely to be drafted and have a serious chance to showcase their skills. Also, since the data in question pertains to the physical capabilities of the most elite athletes on the planet, the spread of the data fits in a relatively predictable range. Therefore, having outliers skew the data is not necessarily a major concern of this exercise.  